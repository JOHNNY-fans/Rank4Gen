{
  "Qwen3-8B": [
    "Qwen3-8B \"no-thinking\" is an 8-billion-parameter variant within the Qwen3 series of large language models developed by the Qwen team at Alibaba. Its core feature is the optimization for a \"non-thinking\" mode, designed to deliver fast, direct, and concise responses, making it particularly suitable for application scenarios requiring immediate feedback. This model is part of the Qwen3 series, a key innovation of which is the ability to seamlessly switch between a \"thinking\" mode and a \"non-thinking\" mode. The \"thinking\" mode is specifically designed to handle complex logical reasoning, mathematical, and coding tasks, while the \"non-thinking\" mode is optimized for efficient, general-purpose dialogue. The Qwen3-8B \"no-thinking\" version focuses on the latter, achieving faster response speeds by omitting reasoning information to provide users with more direct answers. As a member of the Qwen3 family, this model inherits many of its strengths, including powerful capabilities in natural language understanding, code generation, multilingual support, and instruction following. It is built upon a solid foundation of extensive pre-training and post-training on vast amounts of data to better align with human preferences. The Qwen3 architecture itself has undergone a series of optimizations, such as the adoption of Grouped Query Attention (GQA) and the SwiGLU activation function, along with improvements in training stability.In summary, the Qwen3-8B \"no-thinking\" model retains the core capabilities of the Qwen3 series while offering an optimized choice for users and applications that prioritize efficiency and directness through its specialized \"non-thinking\" mode.",
    "Qwen3-8B is an open-source large language model developed by Alibaba's Qwen team. It possesses approximately 8 billion parameters and supports a context length of up to 32,768 tokens. The model demonstrates outstanding performance in domains such as multilingual translation, logical reasoning, mathematical computation, and code generation, with a particular strength in hybrid reasoning and efficient task processing. Its unique characteristic is the ability to seamlessly switch between a \"Thinking Mode\" and a \"Non-Thinking Mode\". In Non-Thinking Mode, the model directly outputs the answer, offering faster and more efficient responses, which is ideal for rapid, everyday conversations or simple queries. This approach enhances the user experience by avoiding unnecessary step-by-step thinking. Conversely, when in-depth analysis is needed, the model can be switched to Thinking Mode to engage in step-by-step reasoning, ensuring the accurate resolution of complex problems.",
    "Qwen3-8B is a lightweight large language model from Alibaba's Tongyi Qianwen series, with a parameter scale of approximately 8 billion. Its performance is comparable to that of the previous generation's 14B-scale models, and it supports a context length of 32K. The model employs a hybrid thinking mode, wherein the \"non-thinking\" mode allows the model to bypass complex reasoning processes to directly output fast and concise answers. This mode optimizes response efficiency while maintaining high-quality output, making it suitable for scenarios such as daily Q&A and simple inquiries.",
    "Qwen3-8B is an 8-billion-parameter large language model that has demonstrated strong performance across multiple benchmarks, including language understanding, logical reasoning, code generation, and multilingual processing. Its overall capabilities are comparable to those of leading-edge models with larger parameter counts. The model offers a special inference mode called \"no-thinking.\" In this mode, the model bypasses its internal complex, chain-of-thought process and directly generates the first response that aligns with the probability distribution based on its existing knowledge parameters for a given input prompt. This operational mode trades the depth of step-by-step deduction on complex problems for extremely low response latency. Consequently, it is particularly well-suited for interactive scenarios that demand high real-time performance but involve relatively simple tasks, such as instant Q&A, content drafting, and fluid conversation, providing users with a fast and direct interactive experience.",
    "Qwen3-8B is the latest generation of large language models released by Alibaba's Qwen team, featuring 8.2 billion parameters and a core characteristic of a unique dual-mode architecture. The model can seamlessly switch between \"thinking mode\" and \"no-thinking mode\" within a single model.The thinking mode employs a step-by-step reasoning process, making it suitable for handling complex tasks in mathematics, programming, and logical reasoning. In contrast, the no-thinking mode provides fast and concise responses, optimized for efficient daily conversations and general tasks. In no-thinking mode, Qwen3-8B surpasses the performance of the previous Qwen2.5 instruction model in mathematics, code generation, and commonsense reasoning. It demonstrates exceptional human preference alignment in creative writing, role-playing, multi-turn dialogue, and instruction following. Furthermore, the model natively supports over 100 languages and dialects, possessing strong capabilities in multilingual instruction comprehension and translation. It has a native context length of 32,768 tokens, which can be extended to 131,072 tokens using YaRN technology. This flexible dual-mode design allows Qwen3-8B to dynamically adjust its computational depth based on task requirements, achieving efficient responses while maintaining powerful reasoning capabilities."
  ],
  "Qwen2.5-7B-Instruct": [
    "Qwen2.5-7B-Instruct is an instruction-tuned language model within the Qwen2.5 series, featuring approximately 7 billion parameters and built upon the Transformer architecture. It is optimized for general-purpose natural language understanding and generation tasks. By aligning the pre-trained foundation model with high-quality instruction data, it demonstrates enhanced instruction-following capabilities and more stable performance across diverse scenarios, including dialogue, reasoning, code generation, and multilingual processing. The model supports a long context window and leverages efficient tokenization schemes and training strategies to improve inference efficiency and broaden its knowledge coverage. Furthermore, it is compatible with common deployment methods, such as local inference and various quantization formats, striking an excellent balance between performance, inference speed, and resource consumption. This makes it highly suitable for developers to integrate and customize rapidly at the application layer.",
    "Qwen2.5-7B-Instruct is a member of the Qwen large language model series, open-sourced by Alibaba Cloud. It is an instruction-tuned language model with 7.61 billion parameters. The model features significant enhancements in its knowledge base, as well as its coding and mathematical capabilities, demonstrating outstanding performance across multiple benchmarks. Built on the Transformer architecture, it incorporates modern techniques such as RoPE (Rotary Position Embedding), SwiGLU (Swish-Gated Linear Unit), and RMSNorm (Root Mean Square Layer Normalization). It supports a context length of up to 128K tokens and can generate a maximum of 8K tokens. Furthermore, it possesses multilingual capabilities, covering over 29 languages including Chinese, English, and French. Qwen2.5-7B-Instruct excels in understanding and generating structured data, such as tables and JSON. It also exhibits improved adaptability to diverse system prompts, which enhances its capabilities for role-playing and functioning as a chatbot.",
    "Qwen2.5-7B-Instruct is an instruction-tuned large language model developed by Alibaba Cloud, featuring 7.61 billion parameters. It is built on the Transformer architecture and integrates advanced techniques such as RoPE (Rotary Position Embedding), SwiGLU, RMSNorm, and Attention QKV bias. The model was pre-trained on a massive dataset containing up to 18 trillion tokens, with its capabilities in coding and mathematics being significantly elevated, benefiting from enhancements derived from specialized models in these domains. It supports a context window of up to 128K tokens and can process long-text inputs exceeding 32,768 tokens by leveraging YaRN technology. Functionally, the model achieves notable improvements in instruction following, long-text generation (over 8K tokens), understanding of structured data (e.g., tables), and generation of structured outputs (particularly in JSON format). It also exhibits greater robustness to a diversity of system prompts, which enhances its capabilities for role-playing and chatbot conditioning. The model supports multilingual processing for over 29 languages, including but not limited to Chinese, English, French, Spanish, Portuguese, German, Italian, Russian, Japanese, Korean, Vietnamese, Thai, and Arabic.",
    "Qwen2.5-7B-Instruct is an instruction-tuned causal language model from Alibaba's Qwen2.5 series, featuring approximately 7.61 billion parameters, of which 6.53 billion are non-embedding parameters. The model is based on the Transformer architecture and incorporates Rotary Position Embedding (RoPE), the SwiGLU activation function, RMSNorm for normalization, and Attention QKV bias. Architecturally, it consists of 28 layers, with 28 query attention heads and 4 key-value heads, implementing Grouped-Query Attention (GQA). It supports a full context length of up to 131,072 tokens and can generate a maximum of 8,192 tokens. The model's ability to process long sequences exceeding 32,768 tokens is enhanced through YaRN Rope scaling. Its training process is divided into pre-training and post-training stages to achieve instruction alignment. The model excels in multilingual capabilities, supporting over 29 languages, including Chinese, English, French, Spanish, and others. Key advantages include excellent instruction-following, long-text generation (exceeding 8K tokens), comprehension of structured data like tables, generation of structured outputs in JSON format, and strong robustness to diverse system prompts, making it highly suitable for role-playing and chatbot applications.",
    "Qwen2.5-7B-Instruct is a causal language model developed by Alibaba, belonging to the Qwen2.5 model series. The model is based on the Transformer architecture and incorporates techniques such as Rotary Position Embedding (RoPE), the SwiGLU activation function, RMSNorm for normalization, and Attention QKV bias. It has a total of 7.61 billion parameters, of which 6.53 billion are non-embedding parameters, and is structured with 28 layers. The Qwen2.5 model series demonstrates performance enhancements in several key areas. Compared to its predecessor, the pre-training dataset for Qwen2.5 was expanded from 7 trillion to 18 trillion tokens, resulting in broader knowledge coverage. Qwen2.5-7B-Instruct shows outstanding multilingual capabilities, significantly outperforming other models in the same parameter class. Concurrently, it has made marked improvements in instruction following, long-text generation, comprehension of structured data (e.g., tables), and the generation of structured outputs (particularly in JSON format). Furthermore, its coding and mathematical abilities have also been enhanced. Notably, a version of Qwen2.5-7B-Instruct with ultra-long context support is available, named Qwen2.5-7B-Instruct-1M. This variant can process a context length of up to 1 million tokens, giving it a distinct advantage in tasks such as long-document analysis."
  ],
  "DeepSeek-R1-Distill-Qwen-7B": [
    "DeepSeek-R1-Distill-Qwen-7B is a lightweight large language model developed by DeepSeek, based on the Qwen2.5-Math-7B architecture. At its core, the model employs knowledge distillation technology, utilizing high-quality Chain-of-Thought (CoT) data generated by the high-performance teacher model, DeepSeek-R1, for direct fine-tuning. This process effectively transfers the thinking patterns required for complex reasoning tasks onto a 7-billion-parameter base. This design enables the model to exhibit performance comparable to frontier large models in mathematics, code generation, and logical reasoning, while maintaining low computational requirements and ease of local deployment, thereby achieving an effective balance between model parameter scale and reasoning capability.",
    "DeepSeek-R1-Distill-Qwen-7B is a 7-billion-parameter language model developed by the DeepSeek team, based on the Qwen2.5-Math-7B architecture and distilled from the \"R1\" teacher model, which was trained via reinforcement learning. Through the distillation process, it retains the reasoning capabilities and Chain-of-Thought (CoT) advantages inherent to the larger R1 model while significantly compressing computational resource requirements, making it ideally suited for deployment on edge devices or resource-constrained servers. The model excels in tasks such as mathematical reasoning, logical analysis, and long-context conversation, and further reduces memory overhead through quantization (e.g., INT8, INT4), thereby achieving a balance between efficiency and performance.",
    "DeepSeek-R1-Distill-Qwen-7B is a lightweight distilled version of DeepSeek-R1, the first-generation reasoning model launched by DeepSeek AI. Utilizing knowledge distillation technology, it condenses the superior capabilities demonstrated by the massive 685-billion-parameter teacher model, DeepSeek-R1, in mathematics, coding, and reasoning tasks into a 7-billion-parameter architecture. Built upon the specialized mathematical reasoning base model, Qwen2.5-Math-7B, this model undergoes supervised fine-tuning using 800,000 high-quality reasoning samples generated by DeepSeek-R1, enabling it to internalize the complex reasoning patterns and Chain-of-Thought (CoT) capabilities of the larger model.",
    "DeepSeek-R1-Distill-Qwen-7B is a 7-billion-parameter dense Transformer language model based on the Qwen2.5-Math-7B architecture. Through knowledge distillation, it extracts advanced reasoning capabilities from the DeepSeek-R1 model, specifically optimized for mathematics, code generation, and complex reasoning tasks. The model achieves an accuracy of 92.8% on the MATH-500 benchmark and excels in tool use, code generation, and overall reasoning efficiency. It demonstrates performance comparable to DeepSeek-R1 across multiple challenging benchmarks and rivals OpenAI-o1 in the domains of mathematics and reasoning. Supporting various inference engines such as vLLM and Transformers, it is released as open source to facilitate access and deployment by the research community on platforms like Hugging Face.",
    "DeepSeek-R1-Distill-Qwen-7B is a lightweight large language model constructed via knowledge distillation technology, transferring the high-performance reasoning capabilities of DeepSeek-R1 (a 685B-parameter reasoning model) onto the Qwen2.5-Math-7B (7B-parameter) architecture. The model achieves a 55.5% Pass@1 score on the AIME 2024 mathematical reasoning benchmark, tripling the reasoning speed compared to the original DeepSeek-R1 while reducing VRAM requirements to one-tenth. It retains DeepSeek-R1’s strengths in mathematical reasoning, code generation, and complex logic processing, while achieving a substantial boost in deployment efficiency through a significant reduction in parameter magnitude. The model supports seamless operation on a single A10 (24G) GPU server; furthermore, 4-bit quantization technology can reduce VRAM usage to under 6GB, enabling small and medium-sized enterprises (SMEs) to deploy enterprise-grade AI reasoning capabilities on consumer-grade hardware. As a fully open-source model, it supports local deployment via tools such as Ollama and vLLM, reducing inference costs from 0.016 CNY to 0.001 CNY per 1,000 tokens, thereby providing an efficient and viable solution for the democratization of AI technology."
  ],
  "gemma-3-12b-it": [
    "Gemma-3-12B-IT is an instruction-tuned version within Google DeepMind's third-generation Gemma model family, featuring a parameter scale of approximately 12 billion (12B). The model boasts an extensive context window, officially supporting up to 128K tokens, enabling it to process ultra-long text or mixed multimodal content. As a multimodal model, it integrates a built-in vision encoder (SigLIP) with approximately 417 million parameters, allowing it to accept combined image and text inputs to generate text outputs. Architecturally, it utilizes a Transformer decoder structure that incorporates Grouped-Query Attention (GQA) and alternating local/global attention layers (at a 5:1 ratio), designed to balance long-context processing with computational efficiency. During training, it employs knowledge distillation strategies (distilled from larger models) to enhance performance and undergoes rigorous safety alignment, including mechanisms for refusal and uncertainty hedging. The model also supports quantization (e.g., INT4, FP8), facilitating deployment on resource-constrained devices. Supporting over 140 languages, Gemma-3-12B-IT offers a lower deployment barrier than larger-scale models and is well-suited for tasks such as dialogue, long-text analysis, Q&A, and multimodal applications on a single GPU (or similar infrastructure).",
    "Gemma-3-12B-IT is a lightweight, open-source multimodal large language model developed by Google, featuring 12 billion parameters. It adopts a decoder-only Transformer architecture and integrates Grouped-Query Attention (GQA), while employing alternating local sliding window self-attention and global self-attention layers to optimize memory efficiency. The model supports a massive context window of 128K tokens (16 times that of previous Gemma generations) and achieves multimodal capabilities through the integration of a customized SigLIP vision encoder, enabling it to process text and image inputs to generate text outputs. Gemma-3-12B-IT offers built-in support for over 140 languages and possesses function calling capabilities, making it suitable for a diverse range of natural language processing and vision-language tasks, including question answering, summarization, reasoning analysis, and image understanding. Furthermore, due to its lightweight design, it can operate efficiently on a single GPU, workstations, laptops, and even mobile devices. The model also provides quantized versions to lower computational requirements and has demonstrated significant improvements in safety evaluations compared to previous generations, particularly regarding child safety, content safety, and representational fairness.",
    "Gemma-3-12b-IT is an open-source model launched by Google in March 2025, featuring 12 billion parameters. It is a multimodal model capable of processing text, images, and short videos. Boasting a context window of 128K tokens, it effectively handles long documents and complex sequential tasks. The model employs an attention mechanism that alternates between local and global attention, significantly optimizing memory usage efficiency. To facilitate deployment, its quantized version drastically reduces video memory (VRAM) requirements from 24GB to 6.6GB, allowing the model to run smoothly on a single GPU. Furthermore, it natively supports over 35 languages, demonstrating exceptional versatility and potential for global applications.",
    "Gemma-3-12b-it is a lightweight, instruction-tuned multimodal language model developed by Google DeepMind, featuring approximately 12 billion parameters. It is specifically designed for efficient deployment on single GPUs, TPUs, or resource-constrained devices such as laptops and smartphones. The model supports a context window of up to 128k tokens, enabling it to effectively handle long inputs, and boasts multilingual capabilities spanning over 140 languages. As a multimodal model, it utilizes a custom SigLIP vision encoder to process text and image inputs and generate text outputs; it is suitable for tasks such as question answering, summarization, reasoning, and image understanding, excelling in benchmarks relative to models of comparable scale. Gemma-3-12b-it represents an advancement in the Gemma family, emphasizing optimized performance, ease of deployment, and enhanced safety features through the introduction of vision-language integration and expanded size options.",
    "Gemma-3-12b-it is an instruction-tuned model within the Gemma 3 series introduced by Google DeepMind, classified as an open-weights multimodal large model with 12 billion parameters (12B). Built upon the same technical architecture as Gemini 2.0, its core characteristic lies in its native support for multimodal inputs (capable of directly understanding and processing text and images), complemented by an extensive 128k token context window, which enables it to excel in long document analysis and image-text interaction tasks. As a \"mid-sized\" model, it strikes a significant balance between reasoning capability, visual understanding, and computational efficiency. Designed for efficient deployment on a single GPU or consumer-grade hardware, and offering multilingual support for over 140 languages, it is well-suited for developing complex AI applications that necessitate a balance between performance and edge resource constraints."
  ],
  "gpt-oss-20b": [
    "GPT-OSS-20B is a lightweight open-weights language model within OpenAI's newly released \"GPT-OSS\" series, specifically designed for low-latency, local deployment, and edge computing scenarios. The model employs an efficient Mixture-of-Experts (MoE) architecture; despite a total parameter count of 21 billion (21B), it activates only approximately 3.6 billion (3.6B) parameters per inference. This design enables it to substantially lower computational resource demands while sustaining high performance, allowing for smooth operation on consumer-grade hardware with merely ~16GB of VRAM. Positioned as a model focused on reasoning and Agentic Capabilities, GPT-OSS-20B excels in code generation, mathematical problem-solving, and complex logical reasoning, achieving benchmark results comparable to OpenAI's o3-mini model. It supports fully transparent Chain-of-Thought (CoT) visualization and configurable reasoning effort. Furthermore, the model natively supports Tool Use and structured outputs and operates under the permissive Apache 2.0 license, granting developers the freedom for commercial use, fine-tuning, and integration, making it an ideal choice for building private AI assistants and efficient edge applications.",
    "gpt-oss-20b is an open-source large language model released by OpenAI, utilizing a Mixture-of-Experts (MoE) architecture. It comprises a total of 21 billion (21B) parameters, with only 3.6 billion (3.6B) active parameters engaged during each forward pass. The model incorporates Grouped Multi-Query Attention, Rotary Positional Embeddings (RoPE), and natively supports a context length of 128k. It undergoes post-training using MXFP4 quantization technology, enabling efficient operation on devices with 16GB of memory. In terms of reasoning capabilities, gpt-oss-20b performs comparably to, or better than, OpenAI o3-mini across standard benchmarks involving coding, competition mathematics, health consulting, and tool calling. The model supports configurable reasoning effort (across low, medium, and high levels), allowing developers to balance latency and performance according to specific use cases. Released under the Apache 2.0 license, it features capabilities such as function calling, tool use, complete Chain-of-Thought (CoT) output, and structured output, and is specifically optimized for low latency, local deployment, and professional applications.",
    "GPT-OSS-20B is an open-weights model released by OpenAI in August 2025. It features a Mixture-of-Experts (MoE) architecture with a total of 21 billion parameters. Utilizing sparse activation technology, it activates only 3.6 billion parameters during task processing, thereby significantly enhancing computational efficiency while maintaining robust performance. The model supports a context length of up to 128K and natively integrates tool calling and Chain-of-Thought reasoning capabilities. Notably, its exceptional optimization enables the model to operate on as little as 16GB of RAM, combining high performance with outstanding deployability, thus making it widely suitable for diverse scenarios ranging from data centers to consumer-grade laptops.",
    "GPT-oss-20b is an open-weights language model developed by OpenAI, marking their first open-source release since GPT-2. Licensed under Apache 2.0, it ensures broad accessibility and customization. Utilizing a compact Mixture-of-Experts (MoE) architecture, the model comprises approximately 21 billion total parameters with 3.6 billion active parameters. Optimized for efficient inference, it requires only 16GB of RAM, enabling deployment on consumer-grade hardware, edge devices, or a single GPU (such as the NVIDIA H100). This text-only model excels in reasoning, mathematical tasks, agentic workflows, and tool-calling capabilities. Its performance across common benchmarks is comparable to models like o3-mini, while supporting low-latency, local, or specialized applications, including fine-tuning for custom scenarios.",
    "gpt-oss-20B is an open-source language model based on the Mixture-of-Experts (MoE) architecture, released by OpenAI in 2025. It features approximately 21 billion total parameters, with about 3.6 billion being active parameters. The model supports a context window of up to 128k tokens and possesses capabilities such as Chain-of-Thought (CoT) reasoning, adjustable reasoning effort, function calling, and tool use. Furthermore, its performance in instruction following and structured output has been reinforced through Supervised Fine-Tuning (SFT) and Reinforcement Learning. Benefiting from the MoE architecture's characteristic of selectively activating only a subset of experts, it significantly reduces computational and video memory requirements while maintaining reasoning performance. This allows for local execution on devices with approximately 16GB of memory, making it suitable for edge deployment and resource-constrained scenarios. In evaluations, the model demonstrates robust performance in reasoning and coding tasks, although there remains room for improvement in its multilingual capabilities. Overall, it is positioned as an efficient, flexible, and locally deployable open-source large language model."
  ],
  "Qwen3-8B-thinking": [
    "The \"Thinking Mode\" of the Qwen3 series is an operational strategy that introduces an explicit \"step-by-step reasoning/Chain-of-Thought\" mechanism during the inference phase. When enabled, the model generates phased intermediate reasoning steps—either internally or explicitly—before providing the final answer, thereby enhancing accuracy and robustness in tasks such as mathematics, logic, programming, and complex commonsense inference. This mode can be dynamically toggled on a per-turn basis by incorporating chat templates, such as <think> tags, within system or user messages, allowing developers to make controllable trade-offs between reasoning quality and response latency/computational budget. Far from being a singular algorithmic modification, Thinking Mode works synergistically with Qwen3's architectural optimizations (e.g., Grouped-Query Attention, long-context support, and inference budget control) to allocate more computational and context resources to problems requiring \"deep thinking.\" In practical engineering terms, it is implemented via two methods: soft switches (template tags) and hard variants, with the model adhering to the most recent thinking instruction during multi-turn sessions.",
    "The \"Thinking Mode\" of Qwen3-8B is a specialized design targeted at complex logical reasoning, mathematics, and coding tasks. Functioning in complementarity with the non-thinking mode, it enables the dynamic adjustment of the model's operational characteristics based on task requirements without the need for model switching. The implementation of this mode is grounded in a four-stage training pipeline, comprising Long Chain-of-Thought (CoT) Cold Start, Reasoning-based Reinforcement Learning (RL), Thinking Mode Fusion, and General Reinforcement Learning. Through this design, Qwen3-8B in Thinking Mode can conduct deep reasoning on intricate problems while achieving flexible control over the thinking budget—allowing users to allocate varying computational resources to the reasoning process according to task complexity, ensuring that harder problems receive extended reasoning time while simpler queries are answered quickly and directly.",
    "Qwen/Qwen3-8B Thinking Mode is a lightweight, 8-billion-parameter model within the Qwen3 series, specifically designed for high-density reasoning tasks. It supports dynamic switching between \"Thinking Mode\" and \"General Mode\": in Thinking Mode, the model explicitly generates a Chain of Thought (CoT) encapsulated within <think> tags, deconstructing complex mathematical, logical, or programming problems step-by-step to achieve higher accuracy. This capability enables it to rival previous-generation models of a significantly larger scale (such as 32B or 72B) in reasoning benchmarks. Conversely, in General Mode, it maintains efficient, instant response speeds suitable for daily dialogue. Benefiting from optimizations in its underlying architecture, the model supports a context window of up to 128k (expandable to 1M) despite its compact 8B parameter size, and possesses robust multilingual instruction-following capabilities, making it an ideal choice for deploying high-performance reasoning applications in resource-constrained environments.",
    "The Thinking Mode of Qwen3-8B represents a core innovation, significantly enhancing the capability to solve complex tasks by simulating the human deep thinking process. In this mode, the model generates detailed intermediate reasoning steps for intricate problems such as mathematical reasoning and code generation; these thinking processes are encapsulated within specific <think> tags, rendering the logical chain clearly visible. Users can actively trigger this mode by inputting the /think command or by setting the enable_thinking=True parameter. This design not only yields a substantial boost in accuracy across professional mathematics and coding benchmarks but also enhances the credibility and interpretability of the model's outputs through transparent reasoning processes, thereby achieving an intelligent balance between \"Slow Thinking, High Precision\" and \"Fast Response, High Efficiency.\"",
    "Qwen3-8B is a large language model developed by the Alibaba Qwen team, featuring a parameter scale of 8 billion and belonging to the Qwen3 series released in 2025. The core innovation of this model lies in its dual-mode architecture, which supports seamless switching between \"Thinking Mode\" and \"Non-Thinking Mode.\" Specifically, the Thinking Mode is designed for complex tasks, enabling the handling of logical reasoning, mathematical calculations, and coding problems through a step-by-step reasoning approach, thereby enhancing accuracy and deep analysis capabilities. In Thinking Mode, the model engages in step-by-step deliberation before generating the final answer, allowing it to excel in addressing challenging problems."
  ],
  "Llama-3.1-8B-Instruct": [
    "Llama-3.1-8B-Instruct is an instruction-tuned language model developed by Meta, serving as a lightweight variant within the Llama 3.1 series. Comprising 8 billion parameters, the model leverages instruction fine-tuning techniques to demonstrate robust task-following capabilities and dialogue understanding, all while maintaining a relatively compact scale. As an open-source model, Llama-3.1-8B-Instruct is optimized for a diverse array of Natural Language Processing (NLP) tasks, including text generation, question answering, summarization, and code understanding. The model employs improved training methodologies and a larger training dataset, resulting in significant enhancements in reasoning capability, knowledge coverage, and multilingual support compared to earlier versions. Thanks to its moderate parameter count, it achieves high inference efficiency without compromising generation quality, making it suitable for deployment in resource-constrained environments and capable of local execution on consumer-grade hardware. Providing flexible customization and integration options for researchers and developers, the model is widely adopted across both industrial and academic research domains.",
    "Llama-3.1-8B-Instruct is an open-source large language model developed by Meta, serving as the instruction-tuned version of the Llama 3.1 series. Featuring a scale of 8 billion parameters, it adopts an optimized autoregressive Transformer architecture. The model is specifically optimized for multilingual dialogue scenarios, supporting eight languages: English, French, German, Italian, Portuguese, Spanish, Hindi, and Thai. With a context window length extended to 128K tokens, it is capable of handling complex instruction following, zero-shot tool calling, and natural language generation tasks. While it may be less reliable than larger models in maintaining multi-turn dialogue combined with tool definitions, it demonstrates efficient performance in human evaluations, outperforming many open-source and closed-source competitors, making it well-suited for domains such as chat agents, text generation, and multilingual applications.",
    "Llama-3.1-8B-Instruct is an instruction-tuned version with 8 billion parameters within the Llama 3.1 series released by Meta, designed to achieve an optimal balance between high performance and computational efficiency through a lightweight footprint. Based on the Transformer architecture, the model was trained on over 15 trillion tokens of high-quality multilingual data. Its most distinctive feature is the extension of the context window to 128k, which significantly enhances capabilities in long document understanding and processing; simultaneously, it natively supports eight languages, including English, German, and French, and has undergone deep optimization for code generation, complex logical reasoning, and Tool Use functions. This positions it as a leader among models of comparable parameter scale, making it highly suitable for low-latency, high-precision local deployment on consumer-grade hardware or edge devices.",
    "Llama-3.1-8B-Instruct is an open-source, instruction-tuned language model with approximately 8 billion parameters, designed for general-purpose dialogue, task execution, and text generation scenarios. Building upon the language understanding and generation capabilities of the base model, it is optimized using large-scale instruction data, enabling it to exhibit greater stability in adhering to user intent, providing structured responses, articulating reasoning, and generalizing across multiple tasks. The model features high reasoning consistency and lower hallucination rates, achieving effective computational efficiency and deployment flexibility while maintaining a compact parameter scale, making it suitable for diverse usage scenarios such as local deployment, API services, and edge inference.",
    "Meta Llama 3.1-8B-Instruct is a member of the Llama 3.1 series released by Meta in July 2024; it is an 8-billion-parameter, instruction-tuned multilingual large language model. Built upon an optimized Transformer architecture, its core features include support for an extended context window of up to 128K tokens, enabling the effective handling of complex tasks such as long-document summarization. The model excels across multiple benchmarks in domains including general knowledge, mathematical reasoning (achieving 84.5% accuracy on GSM-8K), and code generation (attaining a 72.6% pass rate on HumanEval). It also possesses robust multilingual capabilities, proficiently processing languages including English, German, and French. Furthermore, the model natively supports tool calling, enabling interaction with external APIs and applications, thereby extending its scope of practical application and making it ideally suited for constructing efficient chatbots, programming assistants, and multilingual conversational agents."
  ],
  "default": [
    "The Default model is a versatile Large Language Model(LLM) trained on massive datasets. It possesses cross-task comprehension and generation capabilities, enabling it to reason, learn, and make decisions across diverse complex scenarios."
  ],
  "Ministral-3-14B-Instruct-2512": [
    "Ministral-3-14B-Instruct-2512 is the largest model in the Ministral 3 family, offering frontier capabilities and performance comparable to larger models. It is a powerful and efficient multimodal language model with vision capabilities, and this instruct-post-trained version in FP8 precision has been fine-tuned for instruction tasks, making it ideal for chat, instruction following, and assistant-style use cases. The FP8 quantization enables deployment with reduced memory requirements, capable of fitting in 24 GB of VRAM and even less when further quantized. The model supports dozens of major languages including English, French, Spanish, German, Chinese, Japanese, Korean, Arabic and more, and features a large 256 k context window for handling long contexts. It also provides strong adherence to system prompts, native function calling, and structured JSON output, and is licensed under the Apache 2.0 open-source license suitable for both commercial and non-commercial use."
  ],
  "DeepSeek-V3.2": [
    "DeepSeek-V3.2 is an open-source large language model developed by the Chinese AI company DeepSeek and was officially released in December 2025 as the latest version in the DeepSeek-V3 series. The model achieves a strong balance among computational efficiency, reasoning capability, and agent performance through three major technological breakthroughs. First, it introduces the DeepSeek Sparse Attention (DSA) mechanism, an efficient sparse attention method that significantly reduces computational complexity in long-context scenarios while preserving model performance. Second, through a scalable reinforcement learning framework and large-scale post-training computation, the model’s performance reaches the level of GPT-5. Notably, the high-compute variant DeepSeek-V3.2-Speciale even surpasses GPT-5 and matches Gemini-3.0-Pro in reasoning capability, achieving gold-medal-level performance in competitions such as the International Mathematical Olympiad (IMO) and the International Olympiad in Informatics (IOI) in 2025. Third, DeepSeek developed a large-scale agent task synthesis pipeline that enables reasoning to be directly integrated into tool usage, allowing tools to be invoked in both thinking and non-thinking modes. This substantially enhances the model’s generalization ability and instruction-following robustness in complex interactive environments. DeepSeek-V3.2 is well suited for advanced reasoning, agent-based AI applications, tool calling, as well as mathematics and programming tasks. It has been open-sourced on the Hugging Face platform and is widely available through DeepSeek’s App, Web, and API offerings."
  ]
}