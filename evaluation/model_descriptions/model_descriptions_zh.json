{
    "Qwen3-8B": [
        "Qwen3-8B \"no-thinking\" 是由阿里巴巴旗下的千问（Qwen）团队开发的Qwen3系列大型语言模型中的一个80亿参数的变体。其核心特性在于对“非思考”模式的优化，旨在提供快速、直接且简洁的响应，尤其适用于需要即时反馈的应用场景。该模型隶属于Qwen3系列，此系列的一大创新是能够在“思考”模式与“非思考”模式之间进行无缝切换。“思考”模式专为处理复杂的逻辑推理、数学和编码任务而设计，而“非思考”模式则为高效的通用对话进行了优化。Qwen3-8B \"no-thinking\" 版本则侧重于后者，通过省去推理信息，从而实现更快的响应速度，为用户提供更直接的答案。作为Qwen3系列的一员，该模型也继承了其多项优点，包括在自然语言理解、代码生成、多语言支持以及遵循指令等方面的强大能力。它构建于经过大量数据预训练和后训练的坚实基础之上，以更好地对齐人类偏好。Qwen3架构本身也进行了一系列优化，例如采用分组查询注意力（GQA）和SwiGLU激活函数，并在训练稳定性上有所提升。总而言之，Qwen3-8B \"no-thinking\" 模型在保持Qwen3系列核心能力的同时，通过特化其“非思考”模式，为追求效率和直接性的用户及应用提供了一个优化的选择。",
        "Qwen3-8B是由阿里巴巴Qwen团队开发的开源大语言模型，拥有约80亿参数，支持长达32,768个token的上下文处理，在多语言翻译、逻辑推理、数学计算和编码生成等领域表现出色，尤其擅长混合推理和高效任务处理。它独特之处在于可以无缝切换思考模式和no-thinking模式：在no-thinking模式下，模型直接输出答案，响应更快、更高效，适合日常快速对话或简单查询，避免不必要的逐步思考，从而提升用户体验；而在需要深入分析时，可切换到思考模式，进行步步推理，确保复杂问题的准确解决。",
        "Qwen3-8B是阿里巴巴通义千问系列推出的一款轻量级大语言模型，参数规模约为80亿，性能与前代14B规模模型相当，支持32K的上下文长度。该模型采用混合思维模式，其中非思考模式会使模型跳过复杂推理过程，直接输出快速且简洁的答案。此模式优化了响应效率，同时维持高质量输出，适用于日常问答和简单咨询等场景。",
        "Qwen3-8B是一个拥有80亿参数的大型语言模型，它在语言理解、逻辑推理、代码生成及多语言处理等多项基准测试中展现出强劲性能，其综合能力可对标参数量更大的前沿模型。该模型提供一种名为“no-thinking”的特殊推理模式。在此模式下，模型将绕过其内部复杂、链式的思考过程，直接基于其已有的知识参数，对输入提示生成首个符合概率分布的响应。这种工作模式牺牲了在复杂问题上逐步推演的深度，以换取极低的响应延迟。因此，它特别适用于对实时性要求高、但任务相对简单的交互场景，例如即时问答、内容草拟和流畅对话，能够为用户提供快速直接的交互体验。",
        "Qwen3-8B是阿里巴巴Qwen团队发布的最新一代大语言模型,拥有82亿参数,其核心特性在于独特的双模式架构设计。该模型能够在单一模型内无缝切换\"思考模式\"(thinking mode)和\"非思考模式\"(no-thinking mode):思考模式采用逐步推理的方式,适合处理复杂的数学、编程和逻辑推理任务;而非思考模式则提供快速、简洁的响应,专为高效的日常对话和一般性任务优化。在no-thinking模式下,Qwen3-8B在数学、代码生成和常识推理方面的表现超越了之前的Qwen2.5指令模型,在创意写作、角色扮演、多轮对话和指令遵循方面展现出卓越的人类偏好对齐能力。此外,该模型原生支持100多种语言和方言,具备强大的多语言指令理解和翻译能力,原生上下文长度为32,768个tokens,通过YaRN技术可扩展至131,072个tokens。这种灵活的双模式设计使Qwen3-8B能够根据任务需求动态调整计算深度,在保证复杂推理能力的同时实现高效响应。"
    ],
    "Qwen2.5-7B-Instruct": [
        "Qwen/Qwen2.5-7B-Instruct 是一款基于 Transformer 架构的约 70 亿参数指令微调语言模型，属于 Qwen2.5 系列，面向通用自然语言理解与生成任务进行了优化。它在基础大模型的预训练能力之上，通过高质量指令数据进行对齐，使其在对话、推理、代码生成、多语言处理等场景中具备更强的遵循指令能力和更稳定的输出表现。模型通常支持较长上下文输入，并采用高效的分词方案与训练策略来提升推理效率与知识覆盖广度。此外，它兼容常见部署方式（如本地推理和多种量化格式），在性能、推理速度和资源占用之间取得了较好的平衡，适合开发者在应用层快速集成与定制化使用。",
        "Qwen/Qwen2.5-7B-Instruct是阿里巴巴（Alibaba Cloud）开源的通义千问大模型系列中的一员，这是一个具有76.1亿（7.61B）参数的指令微调语言模型。该模型在知识储备、编程和数学能力上均有显著增强，在多项基准测试中表现出色。该模型基于Transformer架构构建，采用了一些如RoPE、SwiGLU、RMSNorm等现代技术。它支持长达128K tokens的上下文长度，并能生成最多8K tokens的内容。此外，它还具备多语言支持能力，涵盖中文、英文、法文等超过29种语言。Qwen2.5-7B-Instruct在理解和生成结构化数据（如表格和JSON）方面表现突出，同时也对多样化的系统提示有更好的适应性，增强了角色扮演和作为聊天机器人的能力。",
        "Qwen2.5-7B-Instruct是阿里云开发的包含76.1亿参数的指令微调大语言模型,采用Transformer架构,集成了RoPE、SwiGLU、RMSNorm和Attention QKV bias等技术。该模型在包含高达18万亿tokens的大规模数据集上进行预训练,在编码和数学领域的能力得到显著提升,得益于这些领域的专业模型增强。该模型支持最多128K tokens的上下文处理,通过YaRN技术可以处理超过32,768 tokens的长文本输入。在功能方面,模型在指令遵循、长文本生成(超过8K tokens)、结构化数据理解(如表格)以及结构化输出生成(特别是JSON格式)方面实现了显著改进,并且对系统提示的多样性更具鲁棒性,增强了角色扮演和聊天机器人条件设置能力。该模型支持超过29种语言的多语言处理,包括中文、英文、法语、西班牙语、葡萄牙语、德语、意大利语、俄语、日语、韩语、越南语、泰语和阿拉伯语等。",
        "Qwen2.5-7B-Instruct是阿里巴巴Qwen2.5系列中的一个指令微调因果语言模型，拥有约7.61亿参数，其中非嵌入参数为6.53亿。该模型基于Transformer架构，采用旋转位置嵌入（RoPE）、SwiGLU激活函数、RMSNorm归一化以及注意力QKV偏置，包含28层、查询注意力头为28个、键值注意力头为4个（使用分组查询注意力GQA）。它支持完整的上下文长度高达131,072个令牌，并可生成最多8,192个令牌，通过YaRN绳缩放增强了对超过32,768个令牌长序列的处理能力。训练过程分为预训练和后训练两个阶段，以实现指令对齐，在多语言能力上表现出色，支持超过29种语言，包括中文、英语、法语、西班牙语等。主要优势包括优秀的指令遵循能力、生成超过8K令牌的长文本、对表格等结构化数据的理解、JSON输出生成，以及对各种系统提示的鲁棒性，使其适用于角色扮演和聊天机器人应用。",
        "Qwen2.5-7B-Instruct是一款由阿里巴巴开发的因果语言模型，属于Qwen2.5模型系列。该模型基于Transformer架构，采用了旋转位置嵌入（RoPE）、SwiGLU激活函数、RMSNorm归一化和注意力QKV偏置等技术。 它的参数总量为76.1亿，其中非嵌入参数为65.3亿，拥有28个层级。Qwen2.5模型系列在多个方面展现了其性能提升。相较于前代，Qwen2.5的预训练数据集规模从7T tokens扩展到了18T tokens，知识覆盖范围更广。 Qwen2.5-7B-Instruct在多语言能力上表现突出，显著优于同量级的其他模型。 同时，它在指令跟随、长文本生成、结构化数据（如表格）理解以及生成结构化输出（尤其是JSON格式）方面都有明显进步。 此外，该模型在代码和数学能力上也得到了增强。值得注意的是，Qwen2.5-7B-Instruct存在支持超长上下文的版本，即Qwen2.5-7B-Instruct-1M，能够处理高达100万词元（tokens）的上下文长度，这使其在处理长文档分析等任务时具备优势。 "
    ],
    "DeepSeek-R1-Distill-Qwen-7B": [
        "DeepSeek-R1-Distill-Qwen-7B 是由深度求索（DeepSeek）基于 Qwen2.5-Math-7B 架构开发的轻量级大语言模型，该模型核心采用了知识蒸馏（Knowledge Distillation）技术，直接利用高性能教师模型 DeepSeek-R1 生成的高质量思维链（Chain-of-Thought）数据进行微调，从而将大模型在复杂推理任务中的思维模式迁移至 70 亿参数规模的基座上；这种设计使得该模型在保持低算力需求和便于本地部署的特性的同时，在数学计算、代码生成及逻辑推理等领域展现出接近前沿大模型的性能，实现了模型参数规模与推理能力的有效平衡。",
        "DeepSeek-R1-Distill-Qwen-7B 是一款由 DeepSeek 团队基于 Qwen2.5-Math-7B 架构，通过强化学习训练的 “R1” 教师模型进行蒸馏而来的 70 亿参数语言模型。它在蒸馏过程中保留了大型模型（R1）所具有的推理能力与链式思维（chain-of-thought）优势，同时大幅压缩计算资源需求，使其更适合在边缘设备或资源有限的服务器上部署。该模型在数学推理、逻辑分析和长文本对话等任务上表现出色，并通过量化（如 INT8、INT4）进一步降低内存开销，兼顾效率与性能。 ",
        "DeepSeek-R1-Distill-Qwen-7B 是 DeepSeek AI 推出的第一代推理模型 DeepSeek-R1 的一个轻量化蒸馏版本，它通过知识蒸馏技术，将庞大的 6850 亿参数教师模型 DeepSeek-R1 在数学、代码和推理任务上表现出的卓越能力浓缩至 70 亿参数的架构中。该模型基于专门的数学推理基座模型 Qwen2.5-Math-7B 构建，并利用了从 DeepSeek-R1 生成的 80 万个高质量推理样本进行有监督微调，使其能够内化大模型的复杂推理模式与思维链（Chain-of-Thought）能力。",
        "DeepSeek-R1-Distill-Qwen-7B是一个7B参数规模的稠密Transformer语言模型，基于Qwen2.5-Math-7B架构，通过知识蒸馏技术从DeepSeek-R1模型中提炼出先进的推理能力，特别优化用于数学、代码生成和复杂推理任务。 该模型在MATH-500基准上达到了92.8%的准确率，并在工具使用、代码生成以及整体推理效率方面表现出色，与DeepSeek-R1在多项困难基准上的性能相当，可媲美OpenAI-o1在数学和推理领域的表现。 它支持多种推理引擎如vLLM和Transformers，并以开源形式发布，便于研究社区在Hugging Face等平台上访问和部署。",
        "DeepSeek-R1-Distill-Qwen-7B是通过知识蒸馏技术构建的轻量化大语言模型，将DeepSeek-R1（685B参数推理模型）的高性能推理能力迁移至Qwen2.5-Math-7B（7B参数）架构上。该模型在AIME 2024数学推理基准测试中达到55.5% Pass@1，推理速度较原始DeepSeek-R1提升3倍，显存需求降低至1/10。它保留了DeepSeek-R1在数学推理、代码生成和复杂逻辑处理方面的优势，同时通过参数量级的显著降低实现了部署效率的大幅提升。模型支持在单张A10(24G)GPU服务器上流畅运行，通过4-bit量化技术可进一步将显存占用降至6GB以下，使中小企业能够在消费级硬件上部署企业级AI推理能力。作为完全开源模型，它支持通过Ollama、vLLM等工具进行本地化部署，推理成本从每千Token 0.016元降至0.001元，为AI技术的普惠化应用提供了高效可行的解决方案。"
    ],
    "gemma-3-12b-it": [
        "Gemma-3-12B-IT 是 Google DeepMind 推出的第三代 Gemma 模型家族中参数规模为约 120 亿（12B）且经过 指令微调（instruction-tuned） 的版本。该模型具备非常大的上下文窗口，官方支持 128K tokens，能够处理超长文本或图文混合内容。它是一个 多模态 模型，内置约 417M 参数的视觉编码器（SigLIP），可以接受图像 + 文字输入并生成文本输出。架构上采用 Transformer 解码器，引入 Grouped-Query Attention (GQA) 以及交替的本地／全局注意力层（5:1比例），以平衡长上下文处理与计算效率。训练过程中，它使用知识蒸馏策略（从更大模型蒸馏）提升性能，并经过严格安全对齐（包括拒绝、不确定性应答机制等）。该模型还支持 量化（如 INT4、FP8），使其能在资源较低的设备上部署。Gemma-3-12B-IT 支持超过 140 种语言，并且比起更大规模模型具有更低的部署门槛，适合在单 GPU（或类似基础设施）上用于对话、长文本分析、问答、多模态应用等任务。",
        "Gemma-3-12B-IT 是由谷歌开发的轻量级开源多模态大语言模型，拥有120亿参数，采用解码器专用的Transformer架构并集成了分组查询注意力机制（GQA），同时交替使用局部滑动窗口自注意力和全局自注意力层以优化内存效率。该模型支持128K个token的超大上下文窗口（是前代Gemma模型的16倍），并通过集成定制化的SigLIP视觉编码器实现多模态能力，可处理文本和图像输入，生成文本输出。Gemma-3-12B-IT 内置支持超过140种语言，具备函数调用功能，适用于问答、摘要总结、推理分析和图像理解等多种自然语言处理和视觉语言任务，同时因其轻量化设计可在单个GPU、工作站、笔记本乃至移动设备上高效运行。该模型还包含量化版本以降低计算需求，在安全性评估中相比前代模型在儿童安全、内容安全和表征公平性等方面取得显著改进。",
        "Gemma-3-12b-IT是谷歌于2025年3月推出的开源模型，具备120亿参数，是一个支持文本、图像及短视频处理的多模态模型。它拥有128K tokens的上下文窗口，能有效处理长文档和复杂序列任务。该模型采用局部与全局注意力交替的注意力机制，显著优化了内存使用效率。为便于部署，其量化版本可将显存需求从24GB大幅降低至6.6GB，使模型能够在单个GPU上流畅运行。同时，它原生支持超过35种语言，展现出优秀的通用性和全球化应用潜力。",
        "Gemma-3-12b-it是由Google DeepMind开发的轻量级、指令调优的多模态语言模型，拥有约120亿参数，专为在单个GPU、TPU或资源受限设备（如笔记本电脑和智能手机）上高效部署而设计。 它支持高达128个token的上下文窗口，能够有效处理长输入，并具备超过140种语言的多语言能力。 作为多模态模型，它使用自定义SigLIP视觉编码器处理文本和图像输入，生成文本输出，适用于问答、摘要、推理和图像理解等任务，并在同类规模模型的基准测试中表现出色。 Gemma-3-12b-it代表了Gemma家族的进步，通过引入视觉-语言整合和更多尺寸选项，强调优化性能、易部署性和增强的安全特性。",
        "Gemma-3-12b-it 是 Google DeepMind 推出的 Gemma 3 系列中的指令微调（Instruction-Tuned）模型，属于拥有 120 亿参数（12B）的开源权重多模态大模型。该模型基于与 Gemini 2.0 相同的技术架构构建，其核心特性在于原生支持多模态输入（能够直接理解和处理文本及图像），并配备了长达 128k token 的上下文窗口，使其在长文档分析和图文交互任务中表现出色。作为一款“中等尺寸”模型，它在推理能力、视觉理解与计算效率之间取得了显著平衡，旨在通过单张 GPU 或消费级硬件实现高效部署，同时提供覆盖超过 140 种语言的多语言支持，适合用于构建需要兼顾性能与端侧资源限制的复杂AI应用。"
    ],
    "gpt-oss-20b": [
        "GPT-OSS-20B 是 OpenAI 最新发布的“GPT-OSS”系列中的一款轻量级开源权重语言模型，专为低延迟、本地部署及边缘计算场景设计。该模型采用高效的 混合专家（Mixture-of-Experts, MoE）架构，虽然总参数量为 210亿（21B），但每次推理仅激活约 36亿（3.6B） 参数，这使其能够在保持高性能的同时大幅降低计算资源需求，仅需约 16GB 显存即可在消费级硬件上流畅运行。作为一款聚焦推理与代理能力（Agentic Capabilities）的模型，GPT-OSS-20B 在代码生成、数学解题及复杂逻辑推理方面表现出色，基准测试成绩接近 OpenAI 的 o3-mini 模型，并支持全透明的思维链（Chain-of-Thought）展示与可配置的推理强度。此外，该模型原生支持工具调用（Tool Use）和结构化输出，且遵循宽松的 Apache 2.0 许可协议，允许开发者自由商用、微调及集成，是目前构建私有化 AI 助手与高效端侧应用的理想选择。",
        "gpt-oss-20b是由OpenAI发布的开源大语言模型，采用Mixture-of-Experts（MoE）架构，包含21B总参数量，其中每次前向传播仅激活3.6B活跃参数，使用群组多查询注意力（Grouped Multi-Query Attention）、旋转位置编码（RoPE）和原生支持128k上下文长度。该模型采用MXFP4量化技术进行后训练，使其能在16GB内存设备上高效运行。在推理能力上，gpt-oss-20b在编码、竞赛数学、健康咨询和工具调用等标准基准测试中表现与OpenAI o3-mini相当或更优。该模型支持可配置的推理强度（低、中、高三个级别），开发者可以根据具体使用场景在延迟和性能之间进行权衡。模型采用Apache 2.0许可证发布，具备函数调用、工具使用、完整的思维链（Chain-of-Thought）输出和结构化输出等能力，专为低延迟、本地部署及专业用途优化。",
        "GPT-OSS-20B是OpenAI于2025年8月发布的一款开放权重模型，它采用总参数量为210亿的混合专家模型架构，通过稀疏激活技术，在处理任务时仅激活36亿参数，从而在保持强大性能的同时显著提升了计算效率。该模型支持高达128K的上下文长度，并原生集成工具调用与链式思维推理能力。特别值得一提的是，其出色的优化使模型仅需16GB内存即可运行，兼具高性能与卓越的可部署性，能够广泛适用于从数据中心到消费级笔记本电脑的多样化场景。",
        "GPT-oss-20b 是 OpenAI 开发的开源权重语言模型，这是自 GPT-2 以来他们的首个开源发布，采用 Apache 2.0 许可，以实现广泛的可访问性和自定义。 作为一种紧凑的专家混合（MoE）架构，该模型总参数约为 210 亿，活跃参数为 36 亿，针对高效推理进行了优化，仅需 16GB 内存，即可在消费级硬件、边缘设备或单 GPU（如 NVIDIA H100）上部署。 这个纯文本模型在推理、数学任务、代理工作流和工具调用能力方面表现出色，在常见基准测试中性能可媲美 o3-mini 等模型，同时支持低延迟、本地或专用应用，例如针对自定义场景的微调。",
        "gpt-oss-20B 是 OpenAI 于 2025 年开源发布的约 210 亿参数（其中约 36 亿为激活参数）的专家混合架构语言模型，支持最长 128k 上下文，具备链式思维推理、可调节推理强度、函数调用与工具使用等能力，并经过监督微调与强化学习强化了指令遵从与结构化输出表现。得益于 MoE 架构仅激活部分专家的特性，它在保持推理性能的同时显著降低计算与显存需求，可在约 16GB 内存的设备上本地运行，适合边缘部署与资源受限场景。在评测中，该模型在推理与代码任务上表现稳健，多语言能力仍有提升空间，整体定位为高效、灵活、可本地部署的开源大语言模型。"
    ],
    "Qwen3-8B-thinking": [
        "Qwen3 系列的 **Thinking（思考）模式** 是一种在推理阶段引入显式“逐步推理/链式思考”机制的运行策略：当启用后，模型会在给出最终答案前内部或显式地产生分阶段的中间推理步骤，以提升在数学、逻辑、编程和复杂常识推断等任务上的准确性与鲁棒性；这一模式既可以通过在系统/用户消息里加入诸如 `<think>` 标签的聊天模板来按回合动态切换，从而让开发者在**推理质量**与**响应延迟/计算预算**之间做出可控权衡。Thinking 模式并非单一算法改动，而是与 Qwen3 的架构优化（如分组查询注意力、长上下文支持与推理预算控制等）协同工作，使得模型能把更多计算/上下文资源投入到需要“深思”的问题上——实际工程实现上存在软开关（模板标记）与硬性变体两类使用方式，且在多轮会话中遵循最近一次的思考指令。",
        "Qwen3-8B的thinking模式是一个针对复杂逻辑推理、数学和编码任务的特殊设计，该模式与非thinking模式相辅相成，可以无需切换模型即可根据任务需求动态调整模型的运行特性。该模式的实现基于一个四阶段的训练流程，包括长链式思维链（CoT）冷启动、基于推理的强化学习（RL）、thinking模式融合和通用强化学习。通过这种设计，Qwen3-8B在thinking模式下能够对更复杂的问题进行深层次推理，同时实现思考预算的灵活控制——用户可以根据任务的复杂程度为推理过程分配不同的计算资源，使得更难的问题能够获得更长的推理时间，而简单问题则可以快速直接回答。",
        "Qwen/Qwen3-8B Thinking Mode 是 Qwen3 系列中专为高密度推理任务设计的 80 亿参数轻量级模型。该模型支持在“思考模式”与“通用模式”间进行动态切换：在思考模式下，模型会显式生成包裹在 <think> 标签内的思维链（Chain of Thought），通过逐步拆解复杂的数学、逻辑或编程问题来换取更高的准确率，使其在推理基准测试上能够越级对标上一代更大规模（如 32B 或 72B）的模型；而在通用模式下，则保持高效的即时响应速度以适应日常对话。得益于底层架构的优化，该模型在仅有 8B 参数的情况下支持长达 128k（可扩展至 1M）的上下文窗口，并具备强大的多语言指令遵循能力，是在资源受限环境下部署高性能推理应用的理想选择。",
        "Qwen3-8B的Thinking模式是其核心创新，它通过模拟人类的深度思考过程来显著提升复杂任务的解决能力。在该模式下，模型会为数学推理、代码生成等复杂问题生成详细的中间推理步骤，这些思考过程被封装在特定的<think>标签内，使得逻辑链条清晰可见。用户可通过输入/think指令或设置enable_thinking=True参数来主动触发此模式。这种设计不仅大幅提升了在专业数学和代码评测中的准确率，更通过透明的推理过程增强了模型结果的可信度和可解释性，实现了“慢思考、高精度”与“快响应、高效率”的智能平衡。",
        "Qwen3-8B是由阿里巴巴Qwen团队开发的大型语言模型，参数规模达80亿，属于2025年发布的Qwen3系列。 该模型的核心创新在于其双模式架构，支持在思考模式（Thinking Mode）和非思考模式（Non-Thinking Mode）之间无缝切换，其中思考模式特别针对复杂任务设计，能够通过逐步推理的方式处理逻辑推理、数学计算和编码等问题，从而提升准确性和深度分析能力。 在思考模式下，模型会先进行分步思考，然后输出最终答案，这使得它在处理高难度问题时表现出色。"
    ],
    "Llama-3.1-8B-Instruct": [
        "Llama-3.1-8B-Instruct 是由Meta公司开发的指令微调语言模型，属于Llama 3.1系列的轻量级版本。该模型包含80亿参数，在保持相对较小模型规模的同时，通过指令微调技术使其具有较强的任务遵循能力和对话理解能力。作为一个开源模型，Llama-3.1-8B-Instruct针对各类自然语言处理任务进行了优化，包括文本生成、问答、摘要和代码理解等场景。该模型采用了改进的训练方法和更大的训练数据集，相比早期版本在推理能力、知识覆盖和多语言支持方面都有显著提升。由于参数量适中，该模型在保证生成质量的同时具有较高的推理效率，适合在资源受限的环境中部署，同时也支持在消费级硬件上进行本地运行。作为开源模型，Llama-3.1-8B-Instruct为研究人员和开发者提供了灵活的定制和集成选项，广泛应用于工业界和学术研究领域。",
        "Llama-3.1-8B-Instruct是由Meta开发的开源大型语言模型，属于Llama 3.1系列的指令微调版本，具有80亿参数规模，采用优化的自回归Transformer架构。该模型专为多语言对话场景优化，支持英语、法语、德语、意大利语、葡萄牙语、西班牙语、印地语和泰语等八种语言，上下文窗口长度扩展至128K tokens，能够处理复杂指令跟随、零样本工具调用和自然语言生成任务。尽管在保持多轮对话与工具定义结合时表现不如更大模型可靠，但其在人类评估中显示出高效性能，优于许多开源和闭源竞争对手，适用于聊天代理、文本生成和多语言应用等领域。",
        "Llama-3.1-8B-Instruct是Meta公司发布的Llama 3.1系列中参数规模为80亿的指令微调版本，旨在以轻量级的体量实现高性能与计算效率的最佳平衡。该模型基于Transformer架构，在超过15万亿Token的高质量多语言数据上进行了训练，其最显著的特性是将上下文窗口扩展至128k，极大增强了长文档理解与处理能力；同时，它原生支持包括英语、德语、法语在内的8种语言，并针对代码生成、复杂逻辑推理以及工具调用（Tool Use）功能进行了深度优化，使其在同等参数规模的模型中处于领先地位，非常适合在消费级硬件或边缘设备上进行低延迟、高精度的本地化部署。",
        "Llama-3.1-8B-Instruct 是一款约 80 亿参数规模的开源指令微调语言模型，面向通用对话、任务执行与文本生成场景。它在基础模型的语言理解与生成能力之上，通过大规模指令数据进行优化，使其在遵循用户意图、结构化回答、推理表达以及多任务泛化方面表现更稳定。该模型具有较高的推理一致性、较低的幻觉率，并在保持较小参数规模的同时实现了较好的计算效率与部署灵活性，适用于本地部署、API 服务及边缘推理等多种使用场景。",
        "Meta Llama 3.1-8B-Instruct 是 Meta 公司于 2024年7月发布的 Llama 3.1 系列中的一员，这是一个拥有80亿参数、经过指令调优的多语言大语言模型。它基于优化的 Transformer 架构，核心特性包括支持高达 128K token 的超长上下文窗口，使其能够有效处理长文档摘要等复杂任务。该模型在通用知识、数学推理（如在GSM-8K上达到84.5%准确率）和代码生成（在HumanEval上达到72.6%通过率）等多个基准测试中表现出色，并具备强大的多语言能力，可流畅处理英语、德语、法语等多种语言。此外，该模型还原生支持工具调用，能够与外部API和应用程序交互，从而扩展其实际应用范围，非常适合用于构建高效的聊天机器人、编程助手和多语言对话代理。"
    ],
    "default": [
        "Default模型是一个通用大语言模型（LLM），是由海量数据训练、具备跨任务理解与生成能力，能够在多种复杂场景中进行推理、学习和决策的人工智能模型。"
    ],
    "Ministral-3-14B-Instruct-2512": [
    "Ministral-3-14B-Instruct-2512 是 Mistral AI 推出的大型多模态指令型 AI 模型，是 Ministral 3 系列中参数规模最大的一款，具有约 140 亿参数的语言与视觉融合能力。这个模型在设计时兼顾了高性能和高效率，其整体表现可以与更大规模的同类模型相媲美，同时支持在各种硬件上部署，包括边缘设备和本地服务器等。该模型的权重采用 FP8 量化格式，有助于减少显存占用，使得在单个 24GB GPU 上即可部署运行，进一步量化后对资源的要求更低。Ministral-3-14B-Instruct-2512 是指令微调版本，专门针对聊天、指令执行和助手类工作负载进行了优化，并且具备图像理解能力，可以处理文本和图像输入，实现统一的多模态推理。它支持几十种语言，包括英语、中文、法语、西班牙语、德语、日语、韩语、阿拉伯语等多种主要语言，适合全球应用。模型还拥有大约 256k 的上下文窗口，有利于处理长上下文任务，并原生支持系统提示遵循、函数调用和结构化 JSON 输出等先进特性。此外，该模型采用 Apache-2.0 开源许可，可用于商业和非商业用途。"
    ],
    "DeepSeek-V3.2": [
        "DeepSeek-V3.2是由中国AI公司DeepSeek开发的开源大型语言模型，于2025年12月正式发布，是DeepSeek-V3系列的最新版本。该模型在计算效率、推理能力和代理性能方面实现了显著平衡，主要通过三项关键技术突破：首先，引入DeepSeek Sparse Attention（DSA）机制，这是一种高效的稀疏注意力方法，大幅降低了长上下文场景下的计算复杂度，同时保持了模型性能。其次，通过可扩展的强化学习框架和大规模后训练计算，使模型性能达到GPT-5水平。其中，高计算变体DeepSeek-V3.2-Speciale甚至超越GPT-5，并在推理能力上媲美Gemini-3.0-Pro，并在2025年国际数学奥林匹克（IMO）和国际信息学奥林匹克（IOI）等竞赛中取得金牌级表现。第三，开发了大规模代理任务合成管道，支持将思考直接融入工具使用，并在思考模式与非思考模式下均可调用工具，大幅提升了模型在复杂交互环境中的泛化能力和指令遵循鲁棒性。DeepSeek-V3.2适用于高级推理、代理AI应用、工具调用以及数学、编程等领域，已在Hugging Face平台开源，并通过DeepSeek的App、Web和API广泛可用。"
    ]
}